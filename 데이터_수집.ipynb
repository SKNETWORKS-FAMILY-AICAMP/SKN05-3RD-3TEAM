{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G3eK3L5RCEGE"
   },
   "source": [
    "# 내부 문서 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xKw_SE78CzTM"
   },
   "outputs": [],
   "source": [
    "# 모듈 설치\n",
    "!pip install pymupdf pdfplumber"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g-sBBIimCq-C"
   },
   "source": [
    "### 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DPLGzhkPCDmc"
   },
   "outputs": [],
   "source": [
    "# 추후 git hub 진행 후, github 에서 다운 받는 것으로 수정 요망\n",
    "import gdown\n",
    "file_id = '1CVlJFI-FGAT4cFhPy5KoZO2aav3ZRD1l'\n",
    "download_url = f'https://drive.google.com/uc?id={file_id}'\n",
    "gdown.download(download_url, 'data.zip', quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pdAsfhVqB2Cv"
   },
   "outputs": [],
   "source": [
    "!unzip data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z9fK30iUGXEG"
   },
   "source": [
    "## PDF 텍스트 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MtmZigeREyu9"
   },
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import glob\n",
    "import os\n",
    "\n",
    "\n",
    "def merge_pdfs_to_txt(file_paths, output_file):\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as out_file:\n",
    "        for file_path in file_paths:\n",
    "            print(f\"Processing {file_path}...\")\n",
    "            try:\n",
    "                with fitz.open(file_path) as pdf:\n",
    "                    for page_num in range(len(pdf)):\n",
    "                        text = pdf[page_num].get_text()\n",
    "                        if text.strip():  # 빈 텍스트가 아닐 때만 추가\n",
    "                            out_file.write(text)\n",
    "                            out_file.write(\"\\n\\n\")  # 페이지 구분\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "    print(f\"텍스트 파일 저장 완료: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wqZNGrUoGifG"
   },
   "outputs": [],
   "source": [
    "# 디렉토리 내 모든 PDF 파일 경로\n",
    "file_paths = glob.glob(os.path.join('./raw', \"*.pdf\"))\n",
    "\n",
    "# 출력 TXT 파일 경로\n",
    "output_txt_file = \"merged_texts.txt\"\n",
    "\n",
    "# 함수 실행\n",
    "merge_pdfs_to_txt(file_paths, output_txt_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6HPV0vReG5wr"
   },
   "source": [
    "## 특수문자 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VBzwNIJ4G0Kx"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # 1. HTML 태그 제거\n",
    "    text = re.sub(r\"<[^>]+>\", \"\", text)\n",
    "\n",
    "    # 2. 의미 없는 문양 제거 (###, ***, --- 등)\n",
    "    text = re.sub(r\"(#+|[-*]{2,})\", \"\", text)\n",
    "\n",
    "    # 3. URL 또는 이메일 주소 제거\n",
    "    text = re.sub(r\"http[s]?://\\S+|www\\.\\S+|[\\w\\.-]+@[\\w\\.-]+\", \"\", text)\n",
    "\n",
    "    # 4. 연속된 공백, 탭, 줄바꿈 정리\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    # 5. 구문에 중요한 특수문자와 수식 관련 기호는 그대로 유지\n",
    "    # 미제거 특수문자 : 숫자와 관련된 특수문자, 쉼표, 마침표\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "# 병합된 텍스트 파일 읽기\n",
    "with open(output_txt_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    merged_text = f.read()\n",
    "\n",
    "# 전처리 실행\n",
    "cleaned_text = preprocess_text(merged_text)\n",
    "print(cleaned_text)\n",
    "\n",
    "# 전처리된 텍스트를 파일로 저장\n",
    "preprocessed_txt_file = \"./preprocessed_texts.txt\"\n",
    "with open(preprocessed_txt_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(cleaned_text)\n",
    "\n",
    "print(f\"전처리된 텍스트 파일 저장 완료: {preprocessed_txt_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WrTzG_edHY_P"
   },
   "source": [
    "# 외부 문서 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AVmOLJIrHb07"
   },
   "outputs": [],
   "source": [
    "!pip install jq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9r797AwPJ7_k"
   },
   "source": [
    "## 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vi-BotDwICaX"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# 압축 파일 해제\n",
    "# 검사할 폴더 경로\n",
    "folder_path = './'  # 폴더 경로로 수정\n",
    "zip_file_path = './Other.zip'  # 압축 파일 경로\n",
    "\n",
    "# 압축 파일 해제\n",
    "try:\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        extract_path = os.path.splitext(zip_file_path)[0]  # 압축 해제 경로\n",
    "        zip_ref.extractall(extract_path)\n",
    "        print(f\"Extracted: {zip_file_path} to {extract_path}\")\n",
    "    # 압축 파일 삭제 (원한다면)\n",
    "except Exception as e:\n",
    "    print(f\"Error extracting {zip_file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0QxjWHmJ-v4"
   },
   "source": [
    "## 필요 없는 키워드 파일 삭제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5mKldBxYM_Jr"
   },
   "source": [
    "### 1차 소거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AGWd4h60JxGc"
   },
   "outputs": [],
   "source": [
    "# # 제거할 키워드\n",
    "# keywords = ['가사', '형사', '특허', '저작권', '상법']\n",
    "\n",
    "# # 검사할 폴더 경로\n",
    "# folder_path = './Other/QA데이터'\n",
    "\n",
    "\n",
    "# # 폴더 내의 모든 파일을 순회\n",
    "# for root, dirs, files in os.walk(folder_path):\n",
    "#     for filename in files:\n",
    "#         file_path = os.path.join(root, filename)\n",
    "#         if filename.endswith('.json'):\n",
    "#             try:\n",
    "#                 # JSON 파일 읽기\n",
    "#                 with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#                     data = json.load(f)\n",
    "\n",
    "#                 # JSON 내용에서 텍스트 추출\n",
    "#                 content = []\n",
    "\n",
    "#                 def extract_text(data):\n",
    "#                     if isinstance(data, dict):\n",
    "#                         for value in data.values():\n",
    "#                             extract_text(value)\n",
    "#                     elif isinstance(data, list):\n",
    "#                         for item in data:\n",
    "#                             extract_text(item)\n",
    "#                     else:\n",
    "#                         content.append(str(data))\n",
    "\n",
    "#                 extract_text(data)\n",
    "\n",
    "#                 content_str = ' '.join(content)\n",
    "\n",
    "#                 # 키워드가 내용에 포함되어 있는지 확인\n",
    "#                 if any(keyword in content_str for keyword in keywords):\n",
    "#                     # 파일 삭제\n",
    "#                     os.remove(file_path)\n",
    "#                     print(f\"Removed: {file_path}\")\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error processing {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oJSfh83DK-Kt"
   },
   "outputs": [],
   "source": [
    "# # 삭제 후 총 파일 개수 검토\n",
    "\n",
    "# # 폴더 내의 모든 파일 개수 계산 (하위 디렉토리 포함)\n",
    "# file_count = 0\n",
    "\n",
    "# for root, dirs, files in os.walk(folder_path):\n",
    "#     file_count += len(files)\n",
    "\n",
    "# print(f\"총 파일 개수: {file_count}개\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "REfTjp6gNA_v"
   },
   "source": [
    "### 2차 소거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AGdQiwRFMUBk"
   },
   "outputs": [],
   "source": [
    "# #3. 빈출 법을 추출해 지워도 될만한 내용들은 지우기 - 14753개\n",
    "\n",
    "\n",
    "# # 검사할 폴더 경로 (실제 경로로 변경하세요)\n",
    "# folder_path = './Other/QA데이터'\n",
    "\n",
    "# # 단어 빈도를 저장할 딕셔너리\n",
    "# word_counts = defaultdict(int)\n",
    "\n",
    "# # 정규식을 사용하여 '**~~법**'에 해당하는 단어를 찾기 위한 패턴\n",
    "# pattern = re.compile(r'\\b(\\S*법)\\b')\n",
    "\n",
    "# # 폴더 내의 모든 JSON 파일을 순회\n",
    "# for root, dirs, files in os.walk(folder_path):\n",
    "#     for filename in files:\n",
    "#         if filename.endswith('.json'):\n",
    "#             file_path = os.path.join(root, filename)\n",
    "#             try:\n",
    "#                 # JSON 파일 읽기\n",
    "#                 with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#                     data = json.load(f)\n",
    "\n",
    "#                 # JSON 내용에서 텍스트 추출\n",
    "#                 content_list = []\n",
    "\n",
    "#                 def extract_text(data):\n",
    "#                     if isinstance(data, dict):\n",
    "#                         for value in data.values():\n",
    "#                             extract_text(value)\n",
    "#                     elif isinstance(data, list):\n",
    "#                         for item in data:\n",
    "#                             extract_text(item)\n",
    "#                     else:\n",
    "#                         content_list.append(str(data))\n",
    "\n",
    "#                 extract_text(data)\n",
    "\n",
    "#                 # 리스트를 문자열로 합치기\n",
    "#                 content = ' '.join(content_list)\n",
    "\n",
    "#                 # 텍스트에서 '**~~법**'에 해당하는 단어를 찾기\n",
    "#                 matches = pattern.findall(content)\n",
    "#                 for word in matches:\n",
    "#                     word_counts[word] += 1\n",
    "\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# # 빈도수가 100번 이상인 단어들만 필터링하여 정렬\n",
    "# frequent_words = {word: count for word, count in word_counts.items() if count >= 100}\n",
    "# sorted_words = sorted(frequent_words.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# # 결과 출력\n",
    "# print(\"100번 이상 출현한 '**~~법' 단어들:\")\n",
    "# for word, count in sorted_words:\n",
    "#     print(f\"{word}: {count}번\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IAjJhl4QM7B2"
   },
   "outputs": [],
   "source": [
    "# # 2차 소거 '건축법', '학교보건법', '초·중등교육법', '주택임대차보호법' ,'유아교육법', '도로교통법', '사립학교법', '고등교육법'\n",
    "\n",
    "# # 제거할 키워드 리스트\n",
    "# keywords = ['건축법', '학교보건법', '초·중등교육법', '주택임대차보호법' ,'유아교육법', '도로교통법', '사립학교법', '고등교육법']\n",
    "\n",
    "# # 검사할 폴더 경로 (실제 경로로 변경하세요)\n",
    "# folder_path = './Other/QA데이터/' # 폴더 경로로 수정\n",
    "\n",
    "\n",
    "# # 폴더 내의 모든 파일을 순회\n",
    "# for root, dirs, files in os.walk(folder_path):\n",
    "#     for filename in files:\n",
    "#         file_path = os.path.join(root, filename)\n",
    "#         if filename.endswith('.json'):\n",
    "#             try:\n",
    "#                 # JSON 파일 읽기\n",
    "#                 with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#                     data = json.load(f)\n",
    "\n",
    "#                 # JSON 내용에서 텍스트 추출\n",
    "#                 content = []\n",
    "\n",
    "#                 def extract_text(data):\n",
    "#                     if isinstance(data, dict):\n",
    "#                         for value in data.values():\n",
    "#                             extract_text(value)\n",
    "#                     elif isinstance(data, list):\n",
    "#                         for item in data:\n",
    "#                             extract_text(item)\n",
    "#                     else:\n",
    "#                         content.append(str(data))\n",
    "\n",
    "#                 extract_text(data)\n",
    "\n",
    "#                 content_str = ' '.join(content)\n",
    "\n",
    "#                 # 키워드가 내용에 포함되어 있는지 확인\n",
    "#                 if any(keyword in content_str for keyword in keywords):\n",
    "#                     # 파일 삭제\n",
    "#                     os.remove(file_path)\n",
    "#                     print(f\"Removed: {file_path}\")\n",
    "#             except Exception as e:\n",
    "#                 pass # Or handle the exception appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GetC0AMeNLda"
   },
   "outputs": [],
   "source": [
    "# # 폴더 내의 모든 파일 개수 계산 (하위 디렉토리 포함)\n",
    "# file_count = 0\n",
    "\n",
    "# for root, dirs, files in os.walk(folder_path):\n",
    "#     file_count += len(files)\n",
    "\n",
    "# print(f\"총 파일 개수: {file_count}개\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KcjoL_OvNRlj"
   },
   "source": [
    "## json 파일 병합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o_SvS-WCN2ZZ"
   },
   "outputs": [],
   "source": [
    "# 전체 파일 폴더\n",
    "folder_path = './Other/QA데이터'\n",
    "\n",
    "# 합칠 파일의 경로\n",
    "output_file = './Other/combined_data.json'\n",
    "\n",
    "# 초기 데이터셋 구조\n",
    "dataset = {\n",
    "    \"system\": \"규정에 대해 자세하게 설명해줄 수 있는 챗봇입니다.\",\n",
    "    \"train\": []\n",
    "}\n",
    "\n",
    "# 폴더 내의 모든 JSON 파일을 순회하여 train list 에 저장\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.json'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            # JSON 파일 읽기\n",
    "            with open(file_path, 'r', encoding='utf-8') as infile:\n",
    "                data = json.load(infile)\n",
    "\n",
    "                # 데이터가 리스트인 경우 (여러 개의 Q&A 포함)\n",
    "                if isinstance(data, list):\n",
    "                    dataset[\"train\"].extend(data)\n",
    "                # 데이터가 딕셔너리인 경우 (단일 Q&A)\n",
    "                elif isinstance(data, dict):\n",
    "                    dataset[\"train\"].append(data)\n",
    "                else:\n",
    "                    print(f\"Unsupported data format in {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# 결과를 JSON 파일로 저장\n",
    "with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "    json.dump(dataset, outfile, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Combined data has been saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2oxXQ4KOOEAE"
   },
   "outputs": [],
   "source": [
    "# JSON 파일 로드 함수\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)  # JSON 데이터를 Python 객체로 변환\n",
    "    return data\n",
    "\n",
    "# 사용 예시\n",
    "file_path = './Other/QAData_all.json'\n",
    "dataset = load_json(file_path)\n",
    "\n",
    "# 로드된 데이터 출력\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tjRvt3iWOdX8"
   },
   "source": [
    "## 파인 튜닝 실행을 위한 데이터 전처리(QA 변환)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3IcGUs1QOih5"
   },
   "outputs": [],
   "source": [
    "#output1.jsonl 파일 생성\n",
    "list_message = []\n",
    "num_data = len(dataset[\"train\"])\n",
    "\n",
    "for i in range(num_data):\n",
    "    question = dataset[\"train\"][i][\"question\"]\n",
    "    answer = dataset[\"train\"][i][\"answer\"]\n",
    "    message = [\n",
    "        {\"role\": \"system\", \"content\": dataset[\"system\"]},\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "        {\"role\": \"assistant\", \"content\": answer},\n",
    "    ]\n",
    "    list_message.append(message)\n",
    "\n",
    "with open(\"output1.jsonl\", \"w\", encoding='utf-8') as file:\n",
    "    for messages in list_message:\n",
    "        json_line = json.dumps({\"messages\": messages}, ensure_ascii=False)\n",
    "        file.write(json_line + '\\n')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
